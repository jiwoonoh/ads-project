{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c3ea64",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205f4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import densenet121e\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df9f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix all sources of randomness\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cc55ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_as_pil(path):\n",
    "    ds = pydicom.dcmread(path)\n",
    "    img = ds.pixel_array.astype(np.float32)\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "    img = (img * 255).round().astype(np.uint8)\n",
    "    return Image.fromarray(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "465b20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the train mean and std\n",
    "train_mean = 0.5037\n",
    "train_std = 0.2510\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=train_mean, std=train_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c0a05",
   "metadata": {},
   "source": [
    "## Import Data + Model\n",
    "\n",
    "Class 0 \n",
    "Class 1\n",
    "Class 2\n",
    "Class 3 == white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e889b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kyra_1/Downloads/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/Kyra_1/Downloads/.venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/Kyra_1/Downloads/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 19 predictions to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# ─── 1) FIX ALL SOURCES OF RANDOMNESS ────────────────────────────────\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ─── 2) DEVICE ───────────────────────────────────────────────────────\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ─── 3) HARD-CODE TRAINING MEAN & STD ────────────────────────────────\n",
    "TRAIN_MEAN = 0.5037\n",
    "TRAIN_STD  = 0.2510\n",
    "\n",
    "# ─── 4) TRANSFORM FOR TEST IMAGES ────────────────────────────────────\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([TRAIN_MEAN], [TRAIN_STD])\n",
    "])\n",
    "\n",
    "# ─── 5) DATASET & DATALOADER ────────────────────────────────────────\n",
    "class DicomDataset(Dataset):\n",
    "    def __init__(self, items, labels_dict, label2idx, transform):\n",
    "        self.items     = items\n",
    "        self.labels    = labels_dict\n",
    "        self.label2idx = label2idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid, path = self.items[idx]\n",
    "        ds         = pydicom.dcmread(path)\n",
    "        arr        = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "        # normalize to [0,1] using numpy.ptp\n",
    "        arr_min = arr.min()\n",
    "        arr_ptp = np.ptp(arr)\n",
    "        arr_norm = (arr - arr_min) / (arr_ptp + 1e-6)\n",
    "\n",
    "        # convert back to 8-bit and apply transforms\n",
    "        img = Image.fromarray((arr_norm * 255).astype(np.uint8))\n",
    "        img = self.transform(img)\n",
    "\n",
    "        lbl = self.labels[(pid, path)]\n",
    "        return img, self.label2idx[lbl]\n",
    "\n",
    "# ─── 6) LOAD TEST ITEMS & LABELS ────────────────────────────────────\n",
    "local_path_stem = \"/Users/Kyra_1/Desktop/local_ADS_data/physionet.org/files/mimic-cxr/2.1.0/\"\n",
    "df_preds        = pd.read_csv(\"five_epoch_pred.csv\")\n",
    "\n",
    "# build label mapping\n",
    "all_labels   = df_preds['true_label']\n",
    "unique_labels = sorted(all_labels.unique())\n",
    "label2idx     = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "\n",
    "# collect (pid, full_path) and labels_dict\n",
    "test_items  = []\n",
    "labels_dict = {}\n",
    "for _, row in df_preds.iterrows():\n",
    "    pid        = row['subject_id']\n",
    "    other_path = row['dicom_path'].split('2.1.0')[1]\n",
    "    full_path  = os.path.join(local_path_stem, other_path.lstrip('/'))\n",
    "    if not os.path.exists(full_path):\n",
    "        continue\n",
    "    test_items.append((pid, full_path))\n",
    "    labels_dict[(pid, full_path)] = row['true_label']\n",
    "\n",
    "# create DataLoader\n",
    "test_loader = DataLoader(\n",
    "    DicomDataset(test_items, labels_dict, label2idx, test_transform),\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ─── 7) BUILD MODEL & LOAD WEIGHTS ─────────────────────────────────\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "model = densenet121(pretrained=True)\n",
    "# swap first conv to accept 1 channel\n",
    "old = model.features.conv0\n",
    "new = nn.Conv2d(\n",
    "    1, old.out_channels,\n",
    "    old.kernel_size, old.stride, old.padding,\n",
    "    bias=(old.bias is not None)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    new.weight[:] = old.weight.mean(dim=1, keepdim=True)\n",
    "    if old.bias is not None:\n",
    "        new.bias[:] = old.bias\n",
    "model.features.conv0 = new\n",
    "\n",
    "# replace classifier\n",
    "model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# load your trained weights\n",
    "model.load_state_dict(torch.load(\"reproduceable_densenet.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ─── 8) RUN INFERENCE & SAVE TO CSV ────────────────────────────────\n",
    "results = []\n",
    "batch_size = test_loader.batch_size\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, labs) in enumerate(test_loader):\n",
    "        imgs  = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        probs  = torch.softmax(logits, dim=1)\n",
    "        preds  = probs.argmax(dim=1)\n",
    "\n",
    "        for b in range(imgs.size(0)):\n",
    "            global_idx = batch_idx * batch_size + b\n",
    "            pid, path  = test_items[global_idx]\n",
    "            true_lbl   = unique_labels[labs[b].item()]\n",
    "            pred_lbl   = unique_labels[preds[b].item()]\n",
    "            prob_vals  = probs[b].cpu().numpy().tolist()\n",
    "\n",
    "            row = {\n",
    "                \"patient_id\":      pid,\n",
    "                \"full_path\":       path,\n",
    "                \"true_label\":      true_lbl,\n",
    "                \"predicted_label\": pred_lbl,\n",
    "            }\n",
    "            for j, p in enumerate(prob_vals):\n",
    "                row[f\"prob_class_{j}\"] = p\n",
    "\n",
    "            results.append(row)\n",
    "\n",
    "# write out\n",
    "out_df = pd.DataFrame(results)\n",
    "out_df.to_csv(\"predictions.csv\", index=False)\n",
    "print(f\"Saved {len(out_df)} predictions to predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2fc5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " L1   = 0.000001, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000030, L2 = 0.000020, max_abs = 0.000015, MAE = 0.000007\n",
      " L1   = 0.000085, L2 = 0.000056, max_abs = 0.000043, MAE = 0.000021\n",
      " L1   = 0.000030, L2 = 0.000021, max_abs = 0.000015, MAE = 0.000008\n",
      " L1   = 0.000031, L2 = 0.000021, max_abs = 0.000016, MAE = 0.000008\n",
      " L1   = 0.000002, L2 = 0.000002, max_abs = 0.000001, MAE = 0.000001\n",
      " L1   = 0.000002, L2 = 0.000001, max_abs = 0.000001, MAE = 0.000001\n",
      " L1   = 0.000029, L2 = 0.000019, max_abs = 0.000014, MAE = 0.000007\n",
      " L1   = 0.000001, L2 = 0.000001, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000043, L2 = 0.000026, max_abs = 0.000022, MAE = 0.000011\n",
      " L1   = 0.000001, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000002, L2 = 0.000001, max_abs = 0.000001, MAE = 0.000000\n",
      " L1   = 0.000000, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000000, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000021, L2 = 0.000015, max_abs = 0.000010, MAE = 0.000005\n",
      " L1   = 0.000000, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000001, L2 = 0.000001, max_abs = 0.000001, MAE = 0.000000\n",
      " L1   = 0.000001, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n",
      " L1   = 0.000000, L2 = 0.000000, max_abs = 0.000000, MAE = 0.000000\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "from_test = pd.read_csv('reproduceable_model_pred.csv')\n",
    "jiwoo_stem = '/Users/jiwoo_noh/Downloads'\n",
    "\n",
    "from_loading = pd.read_csv('predictions.csv')\n",
    "\n",
    "for _, row in from_loading.iterrows():\n",
    "    dicom_id = row['full_path'].split('local_ADS_data')[1]\n",
    "    search_path = jiwoo_stem+ dicom_id\n",
    "    prob_list = [row['prob_class_0'],row['prob_class_1'],row['prob_class_2'],row['prob_class_3']]\n",
    "\n",
    "    mask = from_test['dicom_path'] == search_path\n",
    "    if not mask.any():\n",
    "        print(f\"{search_path!r} not found in from_test\")\n",
    "        continue\n",
    "\n",
    "    # grab the packed‐string out of your test set\n",
    "    raw = from_test.loc[mask, 'probabilities'].iloc[0]\n",
    "\n",
    "    # if it’s a string, turn it into a Python list\n",
    "    if isinstance(raw, str):\n",
    "        raw = ast.literal_eval(raw)\n",
    "\n",
    "    # make numpy arrays\n",
    "    a = np.array(raw, dtype=float)\n",
    "    b = np.array(prob_list, dtype=float)\n",
    "\n",
    "    # element-wise difference: test − pipeline\n",
    "    diffs = a - b\n",
    "\n",
    "    # some simple metrics\n",
    "    l1      = np.sum(np.abs(diffs))         # L₁ norm (sum of abs diffs) - try the mean (torch.mean) so that the loss fluctuates between 0 and 1 and not the entire dimensions \n",
    "    l2      = np.linalg.norm(diffs)         # L₂ norm (Euclidean)\n",
    "    max_abs = np.max(np.abs(diffs))         # maximum absolute difference\n",
    "    mae     = np.mean(np.abs(diffs))        # mean absolute error\n",
    "\n",
    "    #print(\" diffs         :\", diffs.tolist())\n",
    "    print(f\" L1   = {l1:.6f}, L2 = {l2:.6f}, max_abs = {max_abs:.6f}, MAE = {mae:.6f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35503a2c",
   "metadata": {},
   "source": [
    "## Masks\n",
    "\n",
    "Can make more complicated later?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a learnable mask with different initializations to capture different solutions\n",
    "mask = torch.nn.Parameter(torch.rand(1, 1, 224, 224), requires_grad=True)\n",
    "optimizer = torch.optim.Adam([mask], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c64c84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m masks = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mN\u001b[49m):  \u001b[38;5;66;03m# N = number of explanations\u001b[39;00m\n\u001b[32m      4\u001b[39m     mask = torch.nn.Parameter(torch.rand(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m), requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     optimizer = torch.optim.Adam([mask], lr=\u001b[32m1e-2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "masks = []\n",
    "\n",
    "# set the number of different sets you would like\n",
    "\n",
    "for _ in range(10):  \n",
    "    mask = torch.nn.Parameter(torch.rand(1, 1, 224, 224), requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([mask], lr=1e-2)\n",
    "\n",
    "    for i in range(500):\n",
    "        masks.append(mask.detach().clone())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
